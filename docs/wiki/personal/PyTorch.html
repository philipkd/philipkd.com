


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link rel="stylesheet" href="/wiki.css">

<title>PyTorch</title>

</head>

<div class="site-title"><a href="./">Personal Wiki</a> by <a href="https://philipkd.com/">Philip Dhingra</a></div>

<div class="entry">

<div class="page-title">PyTorch</div>

<p><code>model.eval()</code> and <code>model.train()</code> put the model in different modes, where you may want certain features, like drop-out to be on or off</p>

<p><code>model.no_grad()</code> similarly is a toggle, where there is no backpropagation. Set for performance reasons when doing a prediction, since no need to update gradient</p>

<p><code>torch.tensor([1., 2.]).cuda()</code> sends the tensor from CPU to GPU</p>

<p><code>optim.zero_grad()</code> is used for non-RNNs because the .backwards() operation accumulates the gradients already</p>

<h1>Saving Models</h1>

<p><code>torch.save()</code> and <code>torch.load()</code> are standard serialize, deserialize functions that work on any object, from basic dictionaries to model objects</p>

<p><a href="https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch">Use-cases</a>:</p>

<ol>
<li><p>inference: <code>torch.save(model.state_dict())</code>, <code>model.load_state_dict(torch.load(filepath))</code> <em>Note: <code>model.eval()</code> must be run right after so that dropouts are properly configured.</em></p></li>
<li><p>resuming training: you would also need the <code>optimizer.state_dict()</code> and epoch num for a full restore.</p></li>
<li><p>sharing: <code>torch.save(model)</code> <em>Not recommended since as classes and directory structure changes, the references in the model may no longer work</em></p></li>
</ol>

<h2>Models vs model states:</h2>

<ul>
<li>architecture/config</li>
<li>weight values (which were learned during training)</li>
<li>compilation information (if compile()) was called</li>
<li>optimizer and its state, if any (this enables you to restart training where you left)</li>
</ul>

<p><a href="https://www.tensorflow.org/guide/keras/save_and_serialize">source</a> (from tf.keras guide, but still relevant)</p>

<h1>Dimensions</h1>

<p>Dimensions for n-dimensional arrays are not the same as dimensions in real space. In Numpy (which PyTorch uses):</p>

<ul>
<li>A shape with n-comma-separated axes has a dimension of n</li>
<li>the last axis is lowest-level feature</li>
</ul>

<p><em>example 1:</em></p>

<p>M has a shape(4, 3) to represent a list of four "3D points". It has 2 dimensions, and can be described as "4 rows, 3 columns" or "4 by 3." A point in M-space would be something like (1,2), representing the 2nd "point", and its "coordinate on the z-axis."</p>

<p><em>example 2:</em></p>

<p>M has a shape(8,1024,512) to represent 8 images, with 1024 pixels, with 512 color probabilities (from 9 bit space). It has 3 dimensions, and can be described as "8 by 1024 by 512". A point in M-space like (1,2,3) would represent the "2nd image, the 3rd pixel, and the probability estimate for the 4th color in the 512-color space."</p>

<p><code>squeeze</code> removes a 1 from the shape, <code>unsqueeze</code> inserts a 1 into the shape</p>

<p><code>size</code> is the same as <code>shape</code></p>

<br/>
<a href="https://licensebuttons.net/l/by/4.0/"><img src="https://licensebuttons.net/l/by/4.0/80x15.png"></a>