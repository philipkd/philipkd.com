


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link rel="stylesheet" href="/wiki.css">

<title>machine learning</title>

</head>

<div class="site-title"><a href="./">Personal Wiki</a> by <a href="https://philipkd.com/">Philip Dhingra</a></div>

<div class="entry">

<div class="page-title">machine learning</div>

<h1>Model types</h1>

<p>Popular:</p>

<ol>
<li><a href="linear_regression">linear regression</a></li>
<li><a href="convolutional_neural_networks">convolutional neural networks</a></li>
<li><a href="Transformers">Transformers</a></li>
</ol>

<p>In <strong>a recurrent neural network</strong>, y is fed back into itself.</p>

<p><strong>unsupervised learning</strong> takes, for example, MNIST but without labels, and then figures out the obvious classifications. (<a href="https://www.youtube.com/watch?v=rHeaoaiBM6Y">comparison with supervised learning</a>; See also: <strong>self-supervised learning</strong> below)</p>

<p><a href="NLP">NLP</a> models</p>

<h1>Toolchain</h1>

<ol>
<li><a href="GPUs">GPUs</a>, <a href="Python">Python</a>, <a href="pip-conda">pip-conda</a>, <a href="NumPy">NumPy</a></li>
<li><a href="Pandas">Pandas</a>, <a href="PyTorch">PyTorch</a>, <a href="TensorFlow">TensorFlow</a>  </li>
<li><a href="Colab">Colab</a>, <a href="Jupyter">Jupyter</a>, <a href="Kaggle">Kaggle</a> </li>
</ol>

<h1>Introduction</h1>

<p><strong>feedforward neural networks</strong>, or multilayer perceptrons(MLPs), approximate some function in the real-world <em>f</em>. For example, for a real-world classifier, y = <em>f</em>(x) maps an input x to a category y.</p>

<p>A tensor walks through a series of layers (multiple layers = <strong>deep learning</strong>), and at each point finds itself transformed, until it results in some output. Each layer contains a fixed series of operations, typically involving some type of matrix math and some other simple operations, such as square roots or <a href="ReLU">ReLU</a>. The weights of the matrices are the trainable parameters of that layer. In the case of a classifier, that output tensor is then typically softmaxed into a prediction.</p>

<p>Models have two modes. In inference, the input tensor is known, the output tensor is unknown, and the tensor walks through the trained known weights. In training, the input tensor and the output tensors are known, but the weights are unknown.</p>

<h1>Definitions</h1>

<p><strong>cross-entropy</strong> is an always positive, loss-function or "cost"-function to be minimized.</p>

<p><strong>cross-validation (CV)</strong> is a fancy term for splitting samples into validation and training sets.</p>

<p><strong><a href="F1">F1</a></strong></p>

<p><strong>hidden layers</strong> are the layers between input and output. <a href="https://stackoverflow.com/questions/53838304/format-of-adding-hidden-layers-in-keras">source</a></p>

<p><strong>logits</strong> are (-inf,inf) "raw" prediction values, which are usually the last layer, which are then thrown into softmax</p>

<p>A non-informative <strong>log loss</strong> <a href="https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d">is around 0.69</a></p>

<p><strong>mAP (or AP)</strong>: take the precision @ 1 guess, precision @ 2 guesses, ... and average them, then average over all classification types</p>

<p><strong><a href="ReLU">ReLU</a></strong></p>

<p><strong>quantizing</strong> reduces the number of bits used to represent a number, usually from FP32 to INT8</p>

<p><strong>softmax</strong> normalizes inputs that are all over the place, and puts them from [0,1], so that they sum to 1. Gives you the best guess of an answer. It weights answers by exponenting them.</p>

<p><strong>top-k</strong> looks at the accuracy of your highest probability label (top-1 picks the top one, top-3 samples probabilistically from top 3)</p>

<h1>Techniques</h1>

<p><strong>dropout</strong> temporarily inactivates neurons during training so that the remaining sparse network is more responsible for nailing the proper output rather than embedding the noise, thus preventing overfitting.</p>

<p><strong><a href="ensemble_learning">ensemble learning</a></strong></p>

<p><strong><a href="k-folds_validation">k-folds validation</a></strong></p>

<p><strong>k-means clustering</strong> puts data points into <em>k</em> clusters, where sum of squared distances to centroid of each cluster is minimal. You find centroid by a type of shuffling that includes finding nearest point to centroid.</p>

<p><strong><a href="optimizing_machine_learning">optimizing machine learning</a></strong></p>

<p><strong><a href="pseudo-labeling">pseudo-labeling</a></strong></p>

<p><strong><a href="regularization">regularization</a></strong></p>

<p><strong>self-supervised learning</strong> uses a transformation on a sample to predict other transformation (typically identity) of sample. In the case of text, using previous words (subset) to predict next likely words (subset). In the case of an image, jigsawing it (scramble) then figuring out how to put it back together (identity), or grayscaling it (decolorizing), and figuring out how to recolorize it (identity).</p>

<p><strong><a href="simulated_annealing">simulated annealing</a></strong> is based on annealing in metallurgy. You start at an "annealing temperature" and slowly reduce it so the molecules settle into lattice networks that are tighter than if cooled quickly. Likewise, in something like the traveling salesman problem, we take a random route, and modify it randomly. If the modification reduces the cost, we take it. If not we decide with probability <em>p</em> whether to take it. We then gradually reduce <em>p</em>. Simulated annealing starts with favoring <em>exploration</em> and ends with favoring <em>exploitation</em> in its search for an optimal solution. SA is a type of <strong>scheduler</strong> which determines how the learning rate should change over time. Other schedulers include consine annealing which just keeps moving the LR up and down.</p>

<br/>
<a href="https://licensebuttons.net/l/by/4.0/"><img src="https://licensebuttons.net/l/by/4.0/80x15.png"></a>