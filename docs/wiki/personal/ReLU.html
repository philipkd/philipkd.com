


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link rel="stylesheet" href="/wiki.css">

<title>ReLU</title>

</head>

<div class="site-title"><a href="./">Personal Wiki</a> by <a href="https://philipkd.com/">Philip Dhingra</a></div>

<div class="entry">

<div class="page-title">ReLU</div>

<p>ensures outputs are positive by setting negative values to zero</p>

<p>for example: if a filter is fixed, like an edge-finder, then we want it to spit out numbers that map to grayscale. If it's trainable, though, it could spit out numbers that are out of bounds, and so ReLU makes sure it produces usable output.</p>

<p><strong>activation functions</strong> like ReLU, "lets the signal pass through the neuron if the in-signal z is big enough, but limit the output from the neuron if z is not."<a href="https://towardsdatascience.com/part-1-a-neural-network-from-scratch-foundation-e2d119df0f40">*</a></p>

<h1>Examples</h1>

<p>ReLU (rectified linear unit)</p>

<p><img src="relu.png" alt="" /></p>

<p>Sigmoid</p>

<p><img src="sigmoid.png" alt="" /></p>

<br/>
<a href="https://licensebuttons.net/l/by/4.0/"><img src="https://licensebuttons.net/l/by/4.0/80x15.png"></a>